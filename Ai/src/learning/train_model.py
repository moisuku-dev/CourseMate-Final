# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import ast
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, AutoTokenizer # âœ… AutoTokenizer ì‚¬ìš©
from torch.optim import AdamW
from sklearn.model_selection import train_test_split

# ==========================================
# 1. ì„¤ì • (KLUE ëª¨ë¸ë¡œ ë³€ê²½!)
# ==========================================
MODEL_NAME = "klue/bert-base" # âœ… í•œêµ­ì–´ í‘œì¤€ ëª¨ë¸
MAX_LEN = 128
BATCH_SIZE = 32
EPOCHS = 1
LEARNING_RATE = 2e-5
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜: {DEVICE} (GPUê°€ ì—†ìœ¼ë©´ CPUë¡œ ëŒì•„ê°€ì„œ ëŠë¦´ ìˆ˜ ìˆì–´ìš”!)")

# ==========================================
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==========================================
class ReviewDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.df = df
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        row = self.df.iloc[index]
        text = str(row['cleaned_content'])
        try:
            labels = ast.literal_eval(row['label'])
        except:
            labels = [0] * 30 # ì—ëŸ¬ ë°©ì§€ìš© ê¸°ë³¸ê°’

        inputs = self.tokenizer.encode_plus(
            text, None, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', return_token_type_ids=True, truncation=True
        )
        
        return {
            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),
            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),
            'targets': torch.tensor(labels, dtype=torch.float)
        }

# ==========================================
# 3. ëª¨ë¸ ì •ì˜
# ==========================================
class KoBERTClass(nn.Module):
    def __init__(self, num_labels):
        super(KoBERTClass, self).__init__()
        self.bert = BertModel.from_pretrained(MODEL_NAME)
        self.classifier = nn.Linear(768, num_labels)

    def forward(self, input_ids, attention_mask, token_type_ids):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        return self.classifier(output.pooler_output)

# ==========================================
# 4. ì‹¤í–‰ (ë©”ì¸)
# ==========================================
def run_training():
    try:
        df = pd.read_csv('final_dataset_for_ai.csv')
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")
    except:
        print("âŒ 'final_dataset_for_ai.csv' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # íƒœê·¸ ê°œìˆ˜ ìë™ í™•ì¸
    sample_label = ast.literal_eval(df.iloc[0]['label'])
    num_labels = len(sample_label)
    print(f"ğŸ¯ ì˜ˆì¸¡í•  íƒœê·¸ ê°œìˆ˜: {num_labels}ê°œ")

    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (AutoTokenizer ì‚¬ìš©)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    train_dataset = ReviewDataset(train_df, tokenizer, MAX_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    model = KoBERTClass(num_labels)
    model.to(DEVICE)

    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    print("\nğŸ”¥ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (KLUE-BERT)")
    model.train()
    
    for epoch in range(EPOCHS):
        total_loss = 0
        for i, data in enumerate(train_loader):
            ids = data['ids'].to(DEVICE)
            mask = data['mask'].to(DEVICE)
            token_type_ids = data['token_type_ids'].to(DEVICE)
            targets = data['targets'].to(DEVICE)

            outputs = model(ids, mask, token_type_ids)
            loss = criterion(outputs, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            
            if i % 100 == 0: # ë¡œê·¸ ë„ˆë¬´ ë§ì•„ì„œ 100ë²ˆì— í•œë²ˆë§Œ ì¶œë ¥
                print(f"Epoch {epoch+1}/{EPOCHS} | Step {i} | Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(train_loader)
        print(f"âœ… Epoch {epoch+1} ì™„ë£Œ! í‰ê·  Loss: {avg_loss:.4f}")

    torch.save(model.state_dict(), "course_mate_model.pt")
    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨.")

if __name__ == "__main__":
    run_training()