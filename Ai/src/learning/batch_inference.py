# -*- coding: utf-8 -*-
import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertModel, AutoTokenizer
import numpy as np
from tqdm import tqdm  # ì§„í–‰ë¥  ë°” í‘œì‹œ
import json
import os

# ==========================================
# 1. ì„¤ì • (í•™ìŠµ í™˜ê²½ê³¼ ë™ì¼í•˜ê²Œ!)
# ==========================================
MODEL_NAME = "klue/bert-base"  # âœ… KLUE ëª¨ë¸ë¡œ ë³€ê²½ë¨
MAX_LEN = 128
BATCH_SIZE = 64  # ì¶”ë¡ ì€ ë¹ ë¥´ë‹ˆê¹Œ 64ë¡œ ì„¤ì • (ë©”ëª¨ë¦¬ í„°ì§€ë©´ 32ë¡œ ì¤„ì´ì„¸ìš”)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

DATA_FILE = "total_merged_data.csv"   # ì›ë³¸ ë°ì´í„°
MODEL_FILE = "course_mate_model.pt"   # í•™ìŠµëœ ëª¨ë¸ íŒŒì¼
TAGS_FILE = "tags.json"               # íƒœê·¸ ìˆœì„œ íŒŒì¼

# ==========================================
# 2. íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ì•ˆì „ì¥ì¹˜)
# ==========================================
if not os.path.exists(TAGS_FILE):
    print(f"âŒ ì˜¤ë¥˜: '{TAGS_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("   -> create_dataset.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”!")
    exit()

with open(TAGS_FILE, "r", encoding="utf-8") as f:
    FINAL_TAGS = json.load(f)
print(f"âœ… íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ! (ì´ {len(FINAL_TAGS)}ê°œ)")

# ==========================================
# 3. ëª¨ë¸ & ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# ==========================================
class KoBERTClass(nn.Module):
    def __init__(self, num_labels):
        super(KoBERTClass, self).__init__()
        self.bert = BertModel.from_pretrained(MODEL_NAME)
        self.classifier = nn.Linear(768, num_labels)

    def forward(self, input_ids, attention_mask, token_type_ids):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        return self.classifier(output.pooler_output)

class InferenceDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, index):
        text = str(self.texts[index])
        inputs = self.tokenizer.encode_plus(
            text, None, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', return_token_type_ids=True, truncation=True
        )
        return {
            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),
            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long)
        }

# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==========================================
def run_batch_processing():
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... ({DATA_FILE})")
    try:
        df = pd.read_csv(DATA_FILE)
    except:
        print("âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê°€ê²Œ ì´ë¦„ì´ ì—†ìœ¼ë©´ ê·¸ë£¹í™” ë¶ˆê°€ëŠ¥
    if 'store_name' not in df.columns:
        print("âŒ 'store_name' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"ğŸ“Š ì´ {len(df)}ê°œ ë¦¬ë·° ë¶„ì„ ì‹œì‘! (GPU: {DEVICE})")

    # ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = KoBERTClass(len(FINAL_TAGS))
    
    try:
        model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))
    except:
        print("âŒ ëª¨ë¸ íŒŒì¼(.pt)ì´ ì—†ê±°ë‚˜ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
        
    model.to(DEVICE)
    model.eval()

    # ë°ì´í„° ë¡œë” ì¤€ë¹„
    dataset = InferenceDataset(df['content'].tolist(), tokenizer, MAX_LEN)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)

    # ì˜ˆì¸¡ ë£¨í”„
    all_predictions = []
    print("ğŸš€ ì „ì²´ ë¦¬ë·° ë¶„ì„ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”)")
    
    with torch.no_grad():
        for data in tqdm(dataloader):
            ids = data['ids'].to(DEVICE)
            mask = data['mask'].to(DEVICE)
            token_type_ids = data['token_type_ids'].to(DEVICE)

            outputs = model(ids, mask, token_type_ids)
            probs = torch.sigmoid(outputs).cpu().numpy()
            all_predictions.extend(probs)

    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    pred_df = pd.DataFrame(all_predictions, columns=FINAL_TAGS)
    
    # ì›ë³¸ ë°ì´í„°(ê°€ê²Œ ì´ë¦„)ì™€ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©
    result_df = pd.concat([df[['store_name']], pred_df], axis=1)

    print("\nğŸ—ï¸ ì¥ì†Œë³„ íƒœê·¸ ì ìˆ˜ ì§‘ê³„ ì¤‘ (í‰ê·  ì ìˆ˜ ê³„ì‚°)...")
    
    # [í•µì‹¬] ê°™ì€ ì¥ì†Œ(store_name)ë¼ë¦¬ ë¬¶ì–´ì„œ ì ìˆ˜ í‰ê·  ë‚´ê¸°
    spot_scores = result_df.groupby('store_name')[FINAL_TAGS].mean()

    # ì €ì¥
    output_filename = "spot_tag_scores.csv"
    spot_scores.to_csv(output_filename, encoding='utf-8-sig')
    
    print("-" * 30)
    print(f"ğŸ‰ ì‘ì—… ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {output_filename}")
    print("ğŸ‘‰ ì´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•´ì„œ ë°±ì—”ë“œ DB(SPOT_FEATURE)ì— ë„£ìœ¼ì„¸ìš”!")
    print("-" * 30)
    print(spot_scores.head())

if __name__ == "__main__":
    run_batch_processing()